{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55e158f-dcf1-4ded-a9cf-3d0f6fd7637b",
   "metadata": {},
   "source": [
    "### Demo_DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9827af36-4e19-4150-9caf-371c3dd1d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "from collections import deque\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d912805-7ce7-4e19-a3e1-24c844d8a192",
   "metadata": {},
   "source": [
    "### 网络定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6acd0943-0aa9-47ad-a9a1-22bd9b70ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=self.capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        return (\n",
    "            torch.FloatTensor(np.array(states)),\n",
    "            torch.FloatTensor(np.array(actions)),\n",
    "            torch.FloatTensor(np.array(rewards)),\n",
    "            torch.FloatTensor(np.array(next_states)),\n",
    "            torch.FloatTensor(np.array(dones))\n",
    "            )\n",
    "\n",
    "    def save(self, file_path):\n",
    "        np.save(file_path, np.array(self.buffer, dtype=object))  # dtype=object 是关键\n",
    "\n",
    "    def load(self, file_path):\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        self.buffer = deque(data.tolist(), maxlen=self.capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f0418c-5e72-4694-955d-12cb75475e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor: A(s) -> action\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Tanh()  # 限定动作在 [-1, 1]\n",
    "        )\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.model(state) * self.max_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10480abe-ef8a-440e-8d7b-041f7d7dd21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic: Q(s, a) -> y_hat\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        return self.model(torch.cat([state, action], dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b43d3482-8441-4a35-b026-25eae95a0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, max_action,\n",
    "                 actor_lr=1e-3, critic_lr=1e-3, gamma=0.99, tau=0.005, \n",
    "                 device=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") if device is None else device\n",
    "        print(f\"training on: {self.device}\")\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.max_action = max_action\n",
    "\n",
    "        # Actor 网络\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(self.device)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "\n",
    "        # Critic 网络\n",
    "        self.critic = Critic(state_dim, action_dim).to(self.device)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "    def select_action(self, state, noise_std=0.1):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        action = self.actor(state).cpu().data.numpy().flatten()\n",
    "        if noise_std > 0:\n",
    "            action += np.random.normal(0, noise_std, size=action.shape)\n",
    "        return np.clip(action, -self.max_action, self.max_action)\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=64):\n",
    "        if len(replay_buffer) < batch_size:\n",
    "            return\n",
    "\n",
    "        # 抽样 batch\n",
    "        # (s, a, r, s'), r(s, a, s')\n",
    "        states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device).unsqueeze(1)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device).unsqueeze(1)\n",
    "\n",
    "        # 目标Q值: Q_t(s',μ_t(s'))\n",
    "        # target = r + γ * Q_target(s′, μ_target(s′))\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.actor_target(next_states)\n",
    "            target_q = self.critic_target(next_states, next_actions)\n",
    "            target = rewards + self.gamma * (1 - dones) * target_q\n",
    "\n",
    "        # Critic loss\n",
    "        current_q = self.critic(states, actions)\n",
    "        critic_loss = nn.MSELoss()(current_q, target)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Actor loss（最大化 Q，即最小化 -Q）\n",
    "        actor_loss = -self.critic(states, self.actor(states)).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # 软更新 target 网络\n",
    "        self.soft_update(self.actor_target, self.actor)\n",
    "        self.soft_update(self.critic_target, self.critic)\n",
    "\n",
    "    def soft_update(self, target_net, source_net):\n",
    "        for target_param, param in zip(target_net.parameters(), source_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1.0 - self.tau) * target_param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ad1e5-4a8a-4e91-ab26-c0a998c7edfb",
   "metadata": {},
   "source": [
    "### 测试环境选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7716b229-e224-4589-b48d-234c74189bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "# game = \"Pendulum-v1\"\n",
    "game = \"MountainCarContinuous-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db4be324-c8ea-4261-8608-b4d18010d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mujoco series\n",
    "game = \"HalfCheetah-v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e2a2057-a983-47f0-a5fc-dac3835d0f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(game)\n",
    "state_dim = env.observation_space.shape[0]      # 状态维度，例如 3\n",
    "action_dim = env.action_space.shape[0]          # 动作维度，例如 1\n",
    "max_action = float(env.action_space.high[0])    # 最大动作幅度，例如 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee8f55a6-53d9-475e-a3a0-303cb1550704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game: HalfCheetah-v4, state_dim: 17, action_dim: 6, max_action: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"game: {game}, state_dim: {state_dim}, action_dim: {action_dim}, max_action: {max_action}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1f5a87-1dd6-437c-a836-756ffd810687",
   "metadata": {},
   "source": [
    "### 网络定义 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3ea48ad-e5a3-47c9-986b-2a59cae85d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on: cuda\n"
     ]
    }
   ],
   "source": [
    "agent = DDPGAgent(state_dim, action_dim, max_action, actor_lr=5e-4, critic_lr=1e-3, gamma=0.99, tau=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "062d80a2-afef-446c-b9b3-362770a986ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(capacity=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33463a0a-5bb9-4787-b79f-64ed6b970165",
   "metadata": {},
   "source": [
    "超参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9234c98f-c27c-45ec-831f-6f50d414ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(game) # 必要时重构环境\n",
    "num_episodes = 1000\n",
    "batch_size = 256\n",
    "reward_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32797de-fc50-46bb-8bbc-014732cd3c70",
   "metadata": {},
   "source": [
    "不同实验参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe8c42a-67bb-45df-afd0-45811059ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game = \"Pendulum-v1\"\n",
    "max_steps = 300\n",
    "noise_rate = 0.1\n",
    "# warm up not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb33940b-27a2-465e-9c42-3299a20725b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game = \"MountainCarContinuous-v0\"\n",
    "max_steps = 1000\n",
    "noise_rate = 0.6\n",
    "warm_up_round = 50\n",
    "warm_up_steps = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40c5b8fc-e6ca-4cb6-aa97-c6fb9a1722d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# game = \"HalfCheetah-v4\"\n",
    "max_steps = 1000\n",
    "noise_rate = 0.2\n",
    "warm_up_round = 20\n",
    "warm_up_steps = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265741e7-6fc8-4962-9a16-4e3f5fdc6edd",
   "metadata": {},
   "source": [
    "warm up(非必须)\n",
    "\n",
    "对于Delayed Reward + Sparse Success 引导的策略跳跃, 必要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a42316c-8bdf-4fc1-b093-d42ac5754873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warn up for 20 round, sample num: 1000, buffer_len: 100000\n"
     ]
    }
   ],
   "source": [
    "# warm_up round\n",
    "i = 0\n",
    "for i in range(warm_up_round):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    t_inner = 0\n",
    "    while not done and t_inner < max_steps:\n",
    "        action = env.action_space.sample()  # 完全随机\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "    \n",
    "        replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "        state = next_state\n",
    "    \n",
    "        t_inner += 1\n",
    "\n",
    "print(f\"warn up for {i+1} round, sample num: {t_inner}, buffer_len: {len(replay_buffer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0839d5a1-0896-4e61-8959-0798505927b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -444.72, step in this episode 1000\n",
      "Episode 10, Reward: -475.05, step in this episode 1000\n",
      "Episode 20, Reward: 619.07, step in this episode 1000\n",
      "Episode 30, Reward: 1073.15, step in this episode 1000\n",
      "Episode 40, Reward: 375.30, step in this episode 1000\n",
      "Episode 50, Reward: 3195.97, step in this episode 1000\n",
      "Episode 60, Reward: 3473.93, step in this episode 1000\n",
      "Episode 70, Reward: 3335.27, step in this episode 1000\n",
      "Episode 80, Reward: 4020.55, step in this episode 1000\n",
      "Episode 90, Reward: 4657.38, step in this episode 1000\n",
      "Episode 100, Reward: 4851.97, step in this episode 1000\n",
      "Episode 110, Reward: 3000.96, step in this episode 1000\n",
      "Episode 120, Reward: 5428.03, step in this episode 1000\n",
      "Episode 130, Reward: 2460.15, step in this episode 1000\n",
      "Episode 140, Reward: 5578.26, step in this episode 1000\n",
      "Episode 150, Reward: 5538.78, step in this episode 1000\n",
      "Episode 160, Reward: 3551.37, step in this episode 1000\n",
      "Episode 170, Reward: 5871.05, step in this episode 1000\n",
      "Episode 180, Reward: 6186.11, step in this episode 1000\n",
      "Episode 190, Reward: 6302.10, step in this episode 1000\n",
      "Episode 200, Reward: 6756.47, step in this episode 1000\n",
      "Episode 210, Reward: 6454.67, step in this episode 1000\n",
      "Episode 220, Reward: 6884.69, step in this episode 1000\n",
      "Episode 230, Reward: 6716.72, step in this episode 1000\n",
      "Episode 240, Reward: 6931.17, step in this episode 1000\n",
      "Episode 250, Reward: 6859.63, step in this episode 1000\n",
      "Episode 260, Reward: 6585.37, step in this episode 1000\n",
      "Episode 270, Reward: 5972.04, step in this episode 1000\n",
      "Episode 280, Reward: 7383.29, step in this episode 1000\n",
      "Episode 290, Reward: 7444.15, step in this episode 1000\n",
      "Episode 300, Reward: 7486.32, step in this episode 1000\n",
      "Episode 310, Reward: 7922.09, step in this episode 1000\n",
      "Episode 320, Reward: 8099.24, step in this episode 1000\n",
      "Episode 330, Reward: 7571.39, step in this episode 1000\n",
      "Episode 340, Reward: 7868.33, step in this episode 1000\n",
      "Episode 350, Reward: 8279.01, step in this episode 1000\n",
      "Episode 360, Reward: 7988.85, step in this episode 1000\n",
      "Episode 370, Reward: 8621.30, step in this episode 1000\n",
      "Episode 380, Reward: 9059.91, step in this episode 1000\n",
      "Episode 390, Reward: 8440.55, step in this episode 1000\n",
      "Episode 400, Reward: 8446.44, step in this episode 1000\n",
      "Episode 410, Reward: 7850.94, step in this episode 1000\n",
      "Episode 420, Reward: 8544.39, step in this episode 1000\n",
      "Episode 430, Reward: 8615.36, step in this episode 1000\n",
      "Episode 440, Reward: 8740.66, step in this episode 1000\n",
      "Episode 450, Reward: 7075.34, step in this episode 1000\n",
      "Episode 460, Reward: 8716.23, step in this episode 1000\n",
      "Episode 470, Reward: 8670.48, step in this episode 1000\n",
      "Episode 480, Reward: 8297.86, step in this episode 1000\n",
      "Episode 490, Reward: 8091.48, step in this episode 1000\n",
      "Episode 500, Reward: 8555.23, step in this episode 1000\n",
      "Episode 510, Reward: 8991.18, step in this episode 1000\n",
      "Episode 520, Reward: 8699.36, step in this episode 1000\n",
      "Episode 530, Reward: 8993.30, step in this episode 1000\n",
      "Episode 540, Reward: 9073.10, step in this episode 1000\n",
      "Episode 550, Reward: 8586.14, step in this episode 1000\n",
      "Episode 560, Reward: 9324.79, step in this episode 1000\n",
      "Episode 570, Reward: 9199.63, step in this episode 1000\n",
      "Episode 580, Reward: 9032.92, step in this episode 1000\n",
      "Episode 590, Reward: 8903.91, step in this episode 1000\n",
      "Episode 600, Reward: 8877.68, step in this episode 1000\n",
      "Episode 610, Reward: 9046.05, step in this episode 1000\n",
      "Episode 620, Reward: 9031.01, step in this episode 1000\n",
      "Episode 630, Reward: 8985.73, step in this episode 1000\n",
      "Episode 640, Reward: 9272.99, step in this episode 1000\n",
      "Episode 650, Reward: 2828.20, step in this episode 1000\n",
      "Episode 660, Reward: 9294.16, step in this episode 1000\n",
      "Episode 670, Reward: 3479.25, step in this episode 1000\n",
      "Episode 680, Reward: 922.91, step in this episode 1000\n",
      "Episode 690, Reward: 9164.79, step in this episode 1000\n",
      "Episode 700, Reward: 9287.97, step in this episode 1000\n",
      "Episode 710, Reward: 9925.03, step in this episode 1000\n",
      "Episode 720, Reward: 1693.17, step in this episode 1000\n",
      "Episode 730, Reward: 8718.00, step in this episode 1000\n",
      "Episode 740, Reward: 9165.13, step in this episode 1000\n",
      "Episode 750, Reward: 9433.64, step in this episode 1000\n",
      "Episode 760, Reward: 9039.61, step in this episode 1000\n",
      "Episode 770, Reward: 9636.46, step in this episode 1000\n",
      "Episode 780, Reward: 9691.21, step in this episode 1000\n",
      "Episode 790, Reward: 9546.84, step in this episode 1000\n",
      "Episode 800, Reward: 9174.49, step in this episode 1000\n",
      "Episode 810, Reward: 9942.25, step in this episode 1000\n",
      "Episode 820, Reward: 9738.06, step in this episode 1000\n",
      "Episode 830, Reward: 9383.33, step in this episode 1000\n",
      "Episode 840, Reward: 9488.80, step in this episode 1000\n",
      "Episode 850, Reward: 9257.44, step in this episode 1000\n",
      "Episode 860, Reward: 9568.98, step in this episode 1000\n",
      "Episode 870, Reward: 9968.92, step in this episode 1000\n",
      "Episode 880, Reward: 9339.99, step in this episode 1000\n",
      "Episode 890, Reward: 10204.81, step in this episode 1000\n",
      "Episode 900, Reward: 9295.64, step in this episode 1000\n",
      "Episode 910, Reward: 9734.43, step in this episode 1000\n",
      "Episode 920, Reward: 9865.51, step in this episode 1000\n",
      "Episode 930, Reward: 9933.16, step in this episode 1000\n",
      "Episode 940, Reward: 9786.75, step in this episode 1000\n",
      "Episode 950, Reward: 9852.49, step in this episode 1000\n",
      "Episode 960, Reward: 10304.93, step in this episode 1000\n",
      "Episode 970, Reward: 10117.41, step in this episode 1000\n",
      "Episode 980, Reward: 9907.22, step in this episode 1000\n",
      "Episode 990, Reward: 9751.30, step in this episode 1000\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    t = 0\n",
    "    done = False\n",
    "\n",
    "    while not done and t < max_steps:\n",
    "        action = agent.select_action(state, noise_std=noise_rate)  # 加探索噪声\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if t % 5 == 0:\n",
    "            agent.train(replay_buffer, batch_size)\n",
    "        t += 1\n",
    "        \n",
    "    reward_list.append(total_reward)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Reward: {total_reward:.2f}, step in this episide: {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "808d1cf5-05c6-499f-89c9-7a8483bbfc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "print(len(replay_buffer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99abccba-2348-490e-9d44-c945ed6d58bf",
   "metadata": {},
   "source": [
    "### 动画实际演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0efb7de7-2b0d-4108-9849-fb506bc2fe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 50, total_reward: 160.61568894756525\n",
      "t = 100, total_reward: 604.506177932148\n",
      "t = 150, total_reward: 1189.7368938019317\n",
      "t = 200, total_reward: 1808.2301714178893\n",
      "t = 250, total_reward: 2446.8659579739424\n",
      "t = 300, total_reward: 3010.5387280742407\n",
      "t = 350, total_reward: 3623.204237717452\n",
      "t = 400, total_reward: 4207.047516260323\n",
      "t = 450, total_reward: 4826.550915406012\n",
      "t = 500, total_reward: 5514.469879357395\n",
      "t = 550, total_reward: 6146.223352075212\n",
      "t = 600, total_reward: 6787.770211345009\n",
      "t = 650, total_reward: 7456.653418422533\n",
      "t = 700, total_reward: 8058.802065633286\n",
      "t = 750, total_reward: 8683.676346806109\n",
      "t = 800, total_reward: 9294.872777812425\n",
      "t = 850, total_reward: 9945.265529205937\n",
      "t = 900, total_reward: 10584.671331676735\n",
      "t = 950, total_reward: 11204.844287076165\n",
      "t = 1000, total_reward: 11441.490042844776\n",
      "Episode 0, Reward: 11441.490042844776\n"
     ]
    }
   ],
   "source": [
    "# 设置渲染模式为 pygame\n",
    "env = gym.make(game, render_mode=\"human\")\n",
    "# env = gym.make(game)\n",
    "\n",
    "# 关闭 epsilon，完全贪婪策略\n",
    "agent.epsilon = 0.0\n",
    "\n",
    "# 可视化运行一回合\n",
    "num_episodes = 1\n",
    "for ep in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    t = 0\n",
    "\n",
    "    while not done and t < max_steps:\n",
    "    # while not done:\n",
    "        action = agent.select_action(state)\n",
    "        # action = env.action_space.sample()\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        t += 1\n",
    "\n",
    "        # render 使用 pygame 显示界面\n",
    "        env.render()\n",
    "        if t % 50 == 0: \n",
    "            print(f\"t = {t}, total_reward: {total_reward}\")\n",
    "        # time.sleep(0.05)  # 控制每帧速度\n",
    "\n",
    "    print(f\"Episode {ep}, Reward: {total_reward}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d3c775-8845-4d6a-8e5e-ab60c0c074d9",
   "metadata": {},
   "source": [
    "### 保存Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e32d3c71-5361-46b9-887b-ca48e70d0854",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_path = \"data/DDPGgenerated_offline_dataset.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f442a3ef-7acb-492c-8b04-69eae1c6e90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replay buffer saved at: data/DDPGgenerated_offline_dataset.npy, sample num: 1000000\n"
     ]
    }
   ],
   "source": [
    "replay_buffer.save(buffer_path)\n",
    "print(f\"replay buffer saved at: {buffer_path}, sample num: {len(replay_buffer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ed69121-5359-4a42-a130-07fa80bb14f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "new_buffer = ReplayBuffer(capacity=100000)\n",
    "print(len(new_buffer))\n",
    "new_buffer.load(buffer_path)\n",
    "print(len(new_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c4332b7-ed87-4820-ad91-6783b8282c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0.0644,  -0.3848,  -0.5941,  ...,  -0.6222,   0.7229,  -0.8768],\n",
       "         [  0.0525,  -0.0702,   0.3193,  ...,  -7.6673,  -6.6357, -10.9752],\n",
       "         [ -0.0258,  -0.1489,  -0.2472,  ..., -14.6382, -18.7319,  -8.1949],\n",
       "         ...,\n",
       "         [  0.1260,  -0.1067,   0.4556,  ...,  -1.2596, -19.8739, -14.8173],\n",
       "         [  0.0996,   0.2511,   0.1155,  ...,  -3.4950,  19.7792,  -2.8418],\n",
       "         [  0.0940,   0.1420,   0.2458,  ...,  10.8364,  18.7656,  10.8135]]),\n",
       " tensor([[-1.0000,  0.9429, -0.8383, -1.0000, -1.0000, -0.7076],\n",
       "         [ 0.5264,  1.0000,  1.0000,  1.0000, -0.9785, -0.6702],\n",
       "         [ 0.9912, -0.9062,  0.6992, -0.7888, -0.8813, -1.0000],\n",
       "         [-0.8675, -1.0000, -0.6957, -0.6425,  0.9098,  0.6326],\n",
       "         [ 1.0000, -0.4665,  0.8105, -0.3070, -1.0000,  0.9728],\n",
       "         [ 1.0000,  1.0000,  1.0000, -1.0000, -1.0000, -0.7482],\n",
       "         [ 0.9882,  0.7250,  1.0000, -1.0000, -0.8690, -1.0000],\n",
       "         [ 0.9413, -1.0000,  1.0000, -1.0000,  1.0000, -1.0000],\n",
       "         [-1.0000, -0.8836, -1.0000, -0.6211,  0.5987, -1.0000],\n",
       "         [ 1.0000,  1.0000,  1.0000, -0.9927, -0.8911, -1.0000],\n",
       "         [-0.9728, -1.0000, -0.7884,  0.9340,  1.0000,  0.8973],\n",
       "         [-0.7352, -1.0000, -0.9954,  0.7678,  1.0000,  0.8190],\n",
       "         [-0.7338, -0.9186, -0.8677,  1.0000,  0.8022, -0.3991],\n",
       "         [ 1.0000,  0.9672,  0.9210, -0.9616, -1.0000, -1.0000],\n",
       "         [ 0.9239,  1.0000,  1.0000, -0.9680, -0.9869, -1.0000],\n",
       "         [-1.0000,  0.6511, -1.0000,  0.4858,  1.0000,  0.9094],\n",
       "         [ 0.8910,  1.0000,  1.0000, -0.7282, -1.0000, -1.0000],\n",
       "         [ 0.8487,  1.0000,  0.9028, -1.0000, -1.0000, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000,  0.0295,  1.0000, -0.0986],\n",
       "         [-0.3480, -0.3565, -0.7638,  0.3059, -1.0000, -0.9715],\n",
       "         [-0.7922,  1.0000, -1.0000, -0.5347,  0.9976,  0.8545],\n",
       "         [ 0.9825, -0.7706,  1.0000,  0.9819, -1.0000, -1.0000],\n",
       "         [ 0.8333,  1.0000, -0.7947,  0.5727, -0.8631, -0.9153],\n",
       "         [ 0.8988, -0.1279,  0.8055, -0.5819, -1.0000, -0.6585],\n",
       "         [ 0.7159,  0.7957, -0.2956, -1.0000, -0.9786, -1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000, -0.7830,  1.0000,  1.0000],\n",
       "         [ 1.0000,  0.9026,  1.0000, -0.6073, -0.7525, -1.0000],\n",
       "         [-1.0000, -0.8432, -0.8638, -0.6017,  0.9335,  0.9588],\n",
       "         [ 1.0000,  0.9377,  0.6899, -1.0000, -0.6105, -1.0000],\n",
       "         [ 0.7570,  0.9453,  0.8256, -1.0000, -1.0000, -0.8118],\n",
       "         [-0.5499, -0.9374, -1.0000,  1.0000,  1.0000,  1.0000],\n",
       "         [ 0.5279, -0.1704,  0.7879,  1.0000, -0.1816,  1.0000],\n",
       "         [ 1.0000, -0.2049,  1.0000, -0.4068, -0.8333, -1.0000],\n",
       "         [ 0.8063,  1.0000,  0.9076, -1.0000, -1.0000, -0.7201],\n",
       "         [ 0.8156, -0.7788, -1.0000,  0.6241,  0.8365,  0.9941],\n",
       "         [ 1.0000,  1.0000,  1.0000, -1.0000,  1.0000,  1.0000],\n",
       "         [-0.4144, -1.0000, -1.0000,  0.6213,  0.8256,  0.9997],\n",
       "         [-0.9592, -0.9873, -1.0000, -0.3588,  0.7063,  1.0000],\n",
       "         [ 1.0000,  1.0000,  0.7096, -1.0000, -0.9774, -0.6094],\n",
       "         [ 0.7449,  0.9955,  1.0000, -1.0000, -1.0000, -1.0000],\n",
       "         [-0.6258,  0.3452, -0.8475,  0.4645, -0.9847, -0.6328],\n",
       "         [-0.6468,  0.7681, -0.8186, -0.6366,  1.0000,  1.0000],\n",
       "         [-1.0000, -1.0000, -1.0000,  0.4442,  0.8549,  1.0000],\n",
       "         [ 1.0000,  1.0000,  1.0000,  1.0000, -1.0000, -0.9736],\n",
       "         [ 0.7735,  0.1462,  1.0000, -0.8380, -1.0000, -0.9495],\n",
       "         [-0.9604,  1.0000, -0.9703,  0.4808,  0.9955,  0.6919],\n",
       "         [ 1.0000, -0.1906,  1.0000, -0.2876, -1.0000, -0.9763],\n",
       "         [ 1.0000,  0.8267,  0.6700,  0.6954,  0.9108,  1.0000],\n",
       "         [-0.7897, -1.0000, -1.0000, -0.8335,  1.0000, -0.8927],\n",
       "         [ 0.7690, -1.0000,  1.0000,  1.0000, -1.0000,  0.8914],\n",
       "         [ 0.7599, -0.9445,  1.0000, -0.7821, -0.4453, -1.0000],\n",
       "         [ 1.0000,  1.0000,  0.9123, -0.8954, -0.9867, -0.7687],\n",
       "         [ 0.8353, -0.7860,  0.8214, -0.8167, -0.7025, -1.0000],\n",
       "         [ 1.0000,  0.2686,  1.0000, -0.9177, -0.7071, -0.3969],\n",
       "         [ 0.7224, -0.7760,  1.0000, -1.0000, -0.6313, -1.0000],\n",
       "         [ 0.8322,  0.9126,  1.0000, -0.9746, -1.0000, -1.0000],\n",
       "         [-1.0000, -0.9816, -1.0000,  0.8682,  1.0000,  0.9880],\n",
       "         [-1.0000, -1.0000, -0.9803,  0.6238,  0.8546,  0.7373],\n",
       "         [-0.8694, -0.9617, -1.0000, -0.9128,  0.5411,  0.8026],\n",
       "         [ 1.0000,  0.8776, -0.6036, -1.0000, -0.8326, -0.9314],\n",
       "         [-1.0000,  0.8934, -0.8762, -0.7467,  1.0000,  1.0000],\n",
       "         [ 0.9698,  1.0000,  1.0000, -0.5902, -0.7896, -0.6422],\n",
       "         [-0.9596, -0.7401, -1.0000, -0.6541,  0.7342,  1.0000],\n",
       "         [-1.0000, -1.0000, -0.5390, -0.5734, -1.0000, -1.0000]]),\n",
       " tensor([10.0183,  6.7738,  9.2392, 10.4479,  8.7622,  9.9793,  9.8955,  7.0647,\n",
       "          9.4559, 10.2644,  9.7571,  8.5138, 10.6245, 10.0214, 10.1183,  7.3295,\n",
       "          9.4968,  8.9593, 11.4393,  0.7148,  8.6366, 11.7623,  0.0873,  9.9751,\n",
       "          8.3845,  9.7673,  7.4878, 10.0278, 10.8418,  9.7115, 10.8909, 11.6863,\n",
       "         12.6609, 13.1973, -2.6785,  9.7517,  8.1954,  7.0777, 10.4787,  8.7879,\n",
       "          8.6139, 10.3936, 10.1706, 11.6419, 11.1477,  9.6847, 13.1053, 12.4371,\n",
       "          9.8370, 11.0225,  3.5931, 10.2190, 11.7335, 10.0383,  9.1730, -2.2252,\n",
       "          8.3039,  8.3377, 10.8381, 12.6868, 10.7643,  9.0526,  9.7835,  9.0733]),\n",
       " tensor([[ 3.4421e-02, -4.6598e-01, -5.4312e-01,  ..., -8.8837e+00,\n",
       "          -1.6291e+01, -1.0663e+01],\n",
       "         [-1.5142e-02, -1.1947e-01,  6.5860e-01,  ...,  9.0745e+00,\n",
       "          -5.8149e+00,  1.8970e+00],\n",
       "         [-1.1519e-02, -4.1093e-02,  4.4772e-01,  ..., -1.5494e+01,\n",
       "          -1.0890e+01,  1.0763e+00],\n",
       "         ...,\n",
       "         [ 1.1187e-01, -1.6572e-01,  7.3290e-01,  ..., -2.4746e+01,\n",
       "          -3.1893e+00, -6.3118e+00],\n",
       "         [ 1.5609e-01,  3.3483e-01, -5.7710e-01,  ...,  1.7404e+01,\n",
       "          -2.7726e+00,  6.4773e+00],\n",
       "         [-2.4438e-02,  2.7384e-01, -5.8163e-01,  ...,  1.2215e+01,\n",
       "          -1.3435e+00,  1.4035e+00]]),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_buffer.sample(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265823d2-40b9-4379-bfc3-1bc014aac722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
